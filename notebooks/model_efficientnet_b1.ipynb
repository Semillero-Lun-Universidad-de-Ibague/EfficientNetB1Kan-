{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec6f26136ba8484",
   "metadata": {},
   "source": [
    "## Brain Tumor Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7016e2bfc9d25a45",
   "metadata": {},
   "source": [
    "### Clone the Github Repo to access the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6773c0227e9ef390",
   "metadata": {},
   "source": [
    "### Import necessary Libraries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8633cb6bcf8d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision, os, random\n",
    "## from tqdm import tqdm\n",
    "## import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "## import seaborn as sns\n",
    "## import cv2\n",
    "## from IPython.display import Image\n",
    "## import imutils\n",
    "\n",
    "## from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "## import tensorflow.keras as keras\n",
    "## from tensorflow.keras.models import Sequential\n",
    "\n",
    "## import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "## from torchvision.transforms import functional as F\n",
    "## from tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator, array_to_img, img_to_array\n",
    "## from tensorflow.keras.applications import EfficientNetB1\n",
    "## from tensorflow.keras.models import Model\n",
    "## from tensorflow.keras.layers import Flatten, Dense, Conv2D, Dropout, GlobalAveragePooling2D\n",
    "## from tensorflow.keras.optimizers import Adam\n",
    "## from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "## from kcn import ConvNeXtKAN\n",
    "\n",
    "try:\n",
    "    model_name\n",
    "except NameError:\n",
    "    model_name = 'ConvNeXtKAN'\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Cuda is here!\")\n",
    "else:\n",
    "    print(\"Cuda is not here :( \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fadc902c59e10b3",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae40932b51e0016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path_dict = {}\n",
    "\n",
    "# Hope you don't be imprisoned by legacy Python code :)\n",
    "from pathlib import Path\n",
    "\n",
    "# `cwd`: current directory is straightforward\n",
    "cwd = Path.cwd()\n",
    "\n",
    "train_dir = str(cwd) + \"/data/Training\"\n",
    "test_dir = str(cwd) + \"/data/Testing\"\n",
    "\n",
    "data_training = {\n",
    "    'glioma_tumor': [],\n",
    "    'meningioma_tumor': [],\n",
    "    'no_tumor': [],\n",
    "    'pituitary_tumor': []\n",
    "}\n",
    "for i in data_training.keys():\n",
    "    for j in os.listdir(train_dir + \"/\" + i):\n",
    "        data_training[i].append(f\"{train_dir}/{i}/{j}\")\n",
    "\n",
    "data_testing = {\n",
    "    'glioma_tumor': [],\n",
    "    'meningioma_tumor': [],\n",
    "    'no_tumor': [],\n",
    "    'pituitary_tumor': []\n",
    "}\n",
    "for i in data_testing.keys():\n",
    "    for j in os.listdir(test_dir + \"/\" + i):\n",
    "        data_testing[i].append(f\"{test_dir}/{i}/{j}\")\n",
    "\n",
    "len_train = {}\n",
    "for i in data_training.keys():\n",
    "    len_train[i] = len(data_training[i])\n",
    "len_testing = {}\n",
    "for i in data_testing.keys():\n",
    "    len_testing[i] = len(data_testing[i])\n",
    "\n",
    "print({\"training\": len_train, \"testing\": len_testing})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a0bae8737f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17, 17))\n",
    "index = 0\n",
    "for c in data_training.keys():\n",
    "    random.shuffle(data_training[c])\n",
    "    path_list = data_training[c][:5]\n",
    "\n",
    "    for i in range(1, 5):\n",
    "        index += 1\n",
    "        plt.subplot(4, 4, index)\n",
    "        plt.imshow(load_img(path_list[i]))\n",
    "        plt.title(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfd57d19569a1a5",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40021abbbd967eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0, 0.2)),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "num_train = len(train_dataset)\n",
    "split = int(0.8 * num_train)  # 80% training, 20% validation\n",
    "train_size = split\n",
    "valid_size = num_train - split\n",
    "\n",
    "train_dataset, valid_dataset = random_split(train_dataset, [train_size, valid_size])\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\"train: \", len(train_dataset), \"Valid: \", len(valid_dataset), \"test: \", len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2e2336b5e56ff4",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d0c8c3316c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancujte model\n",
    "model = torchvision.models.efficientnet_b1(progress=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "# VytisknÄ›te souhrn modelu\n",
    "def print_model_summary(model, input_size=(3, 128, 128)):\n",
    "    summary(model, input_size=input_size)\n",
    "\n",
    "\n",
    "print_model_summary(model)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64280d094b49878a",
   "metadata": {},
   "source": [
    "### Model Training and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39de9ea8dd689343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.best_score = None\n",
    "        self.early_stop_counter = 0\n",
    "        self.best_model_wts = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "            self.best_model_wts = model.state_dict()\n",
    "        elif val_loss < self.best_score:\n",
    "            self.best_score = val_loss\n",
    "            self.best_model_wts = model.state_dict()\n",
    "            self.early_stop_counter = 0\n",
    "        else:\n",
    "            self.early_stop_counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.early_stop_counter} out of {self.patience}')\n",
    "            if self.early_stop_counter >= self.patience:\n",
    "                if self.verbose:\n",
    "                    print('Early stopping')\n",
    "                model.load_state_dict(self.best_model_wts)\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "# Define the scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.3, patience=2, verbose=True)\n",
    "\n",
    "# Initialize early stopping\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac5fa48028d95c42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T20:14:29.307489Z",
     "start_time": "2024-08-02T20:14:21.289575Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:14:25.195206: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-02 15:14:25.321342: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-02 15:14:25.340893: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-02 15:14:25.539614: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-02 15:14:27.842936: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:16\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "accuracy_training = []\n",
    "accuracy_testing = []\n",
    "loss_validation = []\n",
    "loss_training = []\n",
    "\n",
    "try:\n",
    "    num_epoch\n",
    "except NameError:\n",
    "    num_epochs = 15\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # Training loop with progress bar\n",
    "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for inputs, labels in train_loader:\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.to(torch.device('cuda'))\n",
    "                labels = labels.to(torch.device('cuda'))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(loss=running_loss / (total + 1e-8), accuracy=correct / total)\n",
    "            pbar.update()\n",
    "\n",
    "            # logging \n",
    "            accuracy_training.append(correct / total)\n",
    "            loss_training.append(running_loss / (total + 1e-8))\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = correct / total\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "    accuracy_training.append(epoch_acc)\n",
    "    loss_training.append(epoch_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    # Validation loop with progress bar\n",
    "    with tqdm(total=len(valid_loader), desc='Validation', unit='batch') as pbar:\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = inputs.to(torch.device('cuda'))\n",
    "                    labels = labels.to(torch.device('cuda'))\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                # Update progress bar\n",
    "                pbar.set_postfix(val_loss=val_loss / (val_total + 1e-8), val_accuracy=val_correct / val_total)\n",
    "                pbar.update()\n",
    "\n",
    "    val_loss = val_loss / len(valid_dataset)\n",
    "    val_acc = val_correct / val_total\n",
    "    print(f'Validation Loss: {val_loss:.4f} - Accuracy: {val_acc:.4f}')\n",
    "\n",
    "    loss_validation.append(val_loss)\n",
    "    accuracy_testing.append(val_acc)\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    # Check early stopping\n",
    "    if early_stopping(val_loss, model):\n",
    "        num_epochs = epoch\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8b2032e15a4ba3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T19:32:39.288444Z",
     "start_time": "2024-08-02T19:32:38.722262Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_testing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m accuracy_testing\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy_testing' is not defined"
     ]
    }
   ],
   "source": [
    "from common import save_checkpoint\n",
    "\n",
    "save_checkpoint(model, optimizer, model_name + \"_checkpoint.pth\", num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca5f92c68922db",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1bf81a89513e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import load_checkpoint\n",
    "\n",
    "model, optimizer, num_epochs = load_checkpoint(model, optimizer, model_name + \"_checkpoint.pth\", )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de62fc9b0865c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "epochs = np.arange(0, num_epochs, num_epochs / len(accuracy_training))\n",
    "\n",
    "ax[0].plot(epochs, accuracy_training, 'g', label='Training Accuracy')\n",
    "\n",
    "epochs = np.arange(0, num_epochs, num_epochs / len(accuracy_testing))\n",
    "\n",
    "ax[0].plot(epochs, accuracy_testing, 'y-', label='Validation Accuracy')\n",
    "ax[0].set_title('Model Training & Validation Accuracy')\n",
    "ax[0].legend(loc='lower right')\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "epochs = np.arange(0, num_epochs, num_epochs / len(loss_training))\n",
    "\n",
    "ax[1].plot(epochs, loss_training, 'g', label='Training Loss')\n",
    "\n",
    "epochs = np.arange(0, num_epochs, num_epochs / len(loss_validation))\n",
    "\n",
    "ax[1].plot(epochs, loss_validation, 'y-', label='Validation Loss')\n",
    "ax[1].set_title('Model Training & Validation & Loss')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "\n",
    "plt.savefig(model_name + \".svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98e237bb3685b11",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from common import update_json_with_key\n",
    "\n",
    "update_json_with_key('data.json', model_name, num_epochs, accuracy_training, accuracy_testing, loss_training,\n",
    "                     loss_validation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
